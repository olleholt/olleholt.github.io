[
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "blog",
    "section": "",
    "text": "AQI Analysis in Santa Barbara\n\n\nTime series analysis for Air Quality Index before, during, and after Thomas Fire.\n\n\n\nOlivia Holt\n\n\n\n\n\n\n\n\n\n\n\n\nparquet and GeoParquet\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOlivia Holt\n\n\n\nQuarto\n\n\nR\n\n\nMEDS\n\n\n\neds222\n\n\n\nOlivia Holt\n\n\nDec 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOlivia Holt\n\n\n\nQuarto\n\n\nR\n\n\nMEDS\n\n\n\nEDS242 Final Blog\n\n\n\nOlivia Holt\n\n\nDec 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nHouston Blackout Spatial Analysis\n\n\n\n\n\n\nOlivia Holt\n\n\nFeb 12, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2023-11-06-myfirstpost/index.html",
    "href": "blog/2023-11-06-myfirstpost/index.html",
    "title": "Olivia Holt",
    "section": "",
    "text": "Lake Tahoe water clarity has been measured since 1967.\nClarity is measured as depth using a Secchi disk.\nCalifornia and Nevada, are actively working to restore lake clarity to its historic 97.4 feet.\nFactors known to influence year-to-year changes in clarity include the magnitude of runoff, the warming of the lake surface and the depth to which the lake mixes in the previous winter."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "blog/trial-python-render/contextily-and-parquet.html",
    "href": "blog/trial-python-render/contextily-and-parquet.html",
    "title": "parquet and GeoParquet",
    "section": "",
    "text": "** Apachece Parquet**\n\nimport geopandas\nimport matplotlib.pyplot as plt\n\nimport pystac_client\nimport planetary_computer\n\nimport contextily as ctx # for adding basemaps\n\nModuleNotFoundError: No module named 'contextily'\n\n\n\ncatalog = pystac_client.client.open(\n    \"\",\n    modifier = planetary_computer.sign_inplace,\n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Olivia Holt",
    "section": "",
    "text": "B.S. Environmental Engineering (2020), Colorado School of Mines\nM.S. Hydrologic Science & Engineering (2021), Colorado School of Mines"
  },
  {
    "objectID": "index.html#hi",
    "href": "index.html#hi",
    "title": "Olivia Holt",
    "section": "",
    "text": "Bio here\nB.S. Environmental Engineering (2020), Colorado School of Mines\nM.S. Hydrologic Science & Engineering (2021), Colorado School of Mines"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Olivia Holt",
    "section": "Experience",
    "text": "Experience\nHydrologic Technician for the U.S. Geological Survey\nAssistant Water Resource Engineer for Confluence Engineering Solutions"
  },
  {
    "objectID": "blog/eds242-blog/eds242-finalblogpost.html",
    "href": "blog/eds242-blog/eds242-finalblogpost.html",
    "title": "Olivia Holt",
    "section": "",
    "text": "The escalating threat of climate change has prompted the exploration of innovative solutions, and artificial intelligence (AI) has emerged as a potential ally against environmental problems.\nThe adoption of AI, particularly in the form of machine learning applications, offers promising avenues for combating climate change. AI can contribute to data collection, processing, and analysis related to temperature change, carbon emissions, weather predictions, and the effects of extreme weather events. Additionally, AI applications can optimize energy consumption, transform transportation for efficiency and reduced emissions, monitor ecosystems, predict droughts, enable precision agriculture, and enhance recycling processes. Governments and tech companies alike, including Microsoft, Amazon, and Google, have begun investing in AI programs to combat climate change, recognizing the technology’s potential to revolutionize environmental stewardship.\n\n\nOne company currently working on environmental issues with the use of AI and machine learning is Climate Change AI (CCAI). Stated on their website as part of their guiding principles is, “Diversity, inclusion, and equity are central to the advancement of society in general, and moreover fundamental to progress in addressing climate change. Where possible, it is important that work in climate change and machine learning attempt to address the structural inequities that exist in today’s society” (CCAI). It is crucial for companies to not only address ethical considerations in their statements but to also follow through in their work.\n\n\n\n\n\n\nThe US National Oceanic and Atmospheric Administration leverages artificial intelligence to enhance the accuracy of forecasts for severe weather occurrences, including hurricanes, and to extract valuable insights from diverse datasets.\nScientists at NYU, supported by a $10 million grant from Schmidt Futures, are dedicated to refining climate-change predictions through the enhancement of climate simulations, employing artificial intelligence methodologies.\nThe Green Horizons initiative by IBM, a research endeavor, leverages the capabilities of AI and the Internet of Things (IoT) for the analysis of climate change data. With machine learning, the system assimilates information from diverse sources, including meteorological satellites and traffic cameras, to refine its predictive models. It can predict pollution levels with a 72-hour lead time, offering precise insights on the origin and likely trajectory of pollution down to the nearest kilometer.\n\n\n\n\n\nAccording to technology entrepreneur Ewan Kirk, “Using UAVs to effectively monitor vegetation and land over large areas will help scientists and researchers to create large data sets helping them understand how climate change is affecting some of the world’s most critical resources”. Drones are one of the most efficient ways to collect data from remote regions, and gather information on ecological health.\n\n\n\n\n\nDespite the potential benefits, the application of AI in addressing climate change presents numerous ethical challenges. One of the foremost concerns is the significant energy consumption associated with AI, particularly in training complex models. Data centers that support AI operations often demand substantial electricity, which contributes to environmental issues. The production of electronic devices and AI technologies also involves resource-intensive processes, raising questions about the environmental footprint of the very tools intended to mitigate climate change.\nThe paper “AI for climate: freedom, justice, and other ethical and political Challenges’’ by Mark Coeckelbergh emphasizes the need for increased awareness about the ethical implications of AI usage. They underscore the importance of interdisciplinary education that integrates ethical considerations into technology-related fields. The call for more research highlights the necessity for an interdisciplinary approach to education and transparency within the AI development community.\nThe political implications of employing AI for climate action are complex, with significant emphasis on issues related to freedom and governance. The paper delineates two primary options: the use of AI to influence human behavior through nudging and the “Green Leviathan” approach of AI-driven governance. Nudging, a concept that requires altering the decision-making environment to steer individuals toward climate-friendly choices, raises concerns about preserving human autonomy and rationality. While nudging does not coerce individuals, it influences choices, potentially undermining the principles of autonomy and rational decision-making.\nOn the other hand, the idea of AI-driven governance entails regulating human behavior on a global scale to achieve climate goals. This approach, while effective, poses a threat to individual freedom through coercion. The paper draws parallels to the political philosophy of Thomas Hobbes, who advocated for a Leviathan to prevent chaos, highlighting the ethical dilemma between sacrificing freedom for the greater good or finding a middle ground that preserves both.\nThe global perspective on climate change introduces a complex framework of justice-related challenges. Not all communities and nations are equally vulnerable to climate change, which underlines concerns about justice and fairness. It is necessary to consider the differential impact of climate change measures on various communities, generations, and regions. This introduces questions of global justice, highlighting the ethical responsibility to evaluate AI interventions based on their effects on different populations and geographical locations.\nThe intergenerational dimension of climate change raises ethical questions about who should bear the costs of addressing a crisis that spans multiple generations. As a current graduate student, this prompts reflection on the ethical implications of the actions taken today on the well-being of future generations.\nThe use of AI for climate action holds great potential, but it requires a conscientious approach to ethical and political challenges. Engaging with these issues involves advocating for interdisciplinary education that integrates ethical considerations into technological development. Additionally, fostering transparency within the AI community and contributing to public discussions about the ethical implications of AI applications are crucial steps in navigating the complex terrain of using AI for climate issues. The responsibility extends beyond individual actions to collective efforts that shape the ethical and political landscape of AI in the pursuit of a sustainable and just future.\n\n\n\nCoeckelbergh, M. (2020). AI for climate: Freedom, justice, and other ethical and political challenges. AI and Ethics, 1(1), 67–72. https://doi.org/10.1007/s43681-020-00007-2\nClimate Change AI. (n.d.). https://www.climatechange.ai/about\nMinevich, M. (2023, October 5). AI champions Driving New Industry Solutions for Climate Change. Forbes. https://www.forbes.com/sites/markminevich/2021/03/31/ai-champions-driving-new-industry-solutions-for-climate-change/?sh=4ad1bc274f66"
  },
  {
    "objectID": "blog/eds242-blog/eds242-finalblogpost.html#ai-and-machine-learning-in-climate-analysis",
    "href": "blog/eds242-blog/eds242-finalblogpost.html#ai-and-machine-learning-in-climate-analysis",
    "title": "Olivia Holt",
    "section": "",
    "text": "The escalating threat of climate change has prompted the exploration of innovative solutions, and artificial intelligence (AI) has emerged as a potential ally against environmental problems.\nThe adoption of AI, particularly in the form of machine learning applications, offers promising avenues for combating climate change. AI can contribute to data collection, processing, and analysis related to temperature change, carbon emissions, weather predictions, and the effects of extreme weather events. Additionally, AI applications can optimize energy consumption, transform transportation for efficiency and reduced emissions, monitor ecosystems, predict droughts, enable precision agriculture, and enhance recycling processes. Governments and tech companies alike, including Microsoft, Amazon, and Google, have begun investing in AI programs to combat climate change, recognizing the technology’s potential to revolutionize environmental stewardship.\n\n\nOne company currently working on environmental issues with the use of AI and machine learning is Climate Change AI (CCAI). Stated on their website as part of their guiding principles is, “Diversity, inclusion, and equity are central to the advancement of society in general, and moreover fundamental to progress in addressing climate change. Where possible, it is important that work in climate change and machine learning attempt to address the structural inequities that exist in today’s society” (CCAI). It is crucial for companies to not only address ethical considerations in their statements but to also follow through in their work.\n\n\n\n\n\n\nThe US National Oceanic and Atmospheric Administration leverages artificial intelligence to enhance the accuracy of forecasts for severe weather occurrences, including hurricanes, and to extract valuable insights from diverse datasets.\nScientists at NYU, supported by a $10 million grant from Schmidt Futures, are dedicated to refining climate-change predictions through the enhancement of climate simulations, employing artificial intelligence methodologies.\nThe Green Horizons initiative by IBM, a research endeavor, leverages the capabilities of AI and the Internet of Things (IoT) for the analysis of climate change data. With machine learning, the system assimilates information from diverse sources, including meteorological satellites and traffic cameras, to refine its predictive models. It can predict pollution levels with a 72-hour lead time, offering precise insights on the origin and likely trajectory of pollution down to the nearest kilometer.\n\n\n\n\n\nAccording to technology entrepreneur Ewan Kirk, “Using UAVs to effectively monitor vegetation and land over large areas will help scientists and researchers to create large data sets helping them understand how climate change is affecting some of the world’s most critical resources”. Drones are one of the most efficient ways to collect data from remote regions, and gather information on ecological health.\n\n\n\n\n\nDespite the potential benefits, the application of AI in addressing climate change presents numerous ethical challenges. One of the foremost concerns is the significant energy consumption associated with AI, particularly in training complex models. Data centers that support AI operations often demand substantial electricity, which contributes to environmental issues. The production of electronic devices and AI technologies also involves resource-intensive processes, raising questions about the environmental footprint of the very tools intended to mitigate climate change.\nThe paper “AI for climate: freedom, justice, and other ethical and political Challenges’’ by Mark Coeckelbergh emphasizes the need for increased awareness about the ethical implications of AI usage. They underscore the importance of interdisciplinary education that integrates ethical considerations into technology-related fields. The call for more research highlights the necessity for an interdisciplinary approach to education and transparency within the AI development community.\nThe political implications of employing AI for climate action are complex, with significant emphasis on issues related to freedom and governance. The paper delineates two primary options: the use of AI to influence human behavior through nudging and the “Green Leviathan” approach of AI-driven governance. Nudging, a concept that requires altering the decision-making environment to steer individuals toward climate-friendly choices, raises concerns about preserving human autonomy and rationality. While nudging does not coerce individuals, it influences choices, potentially undermining the principles of autonomy and rational decision-making.\nOn the other hand, the idea of AI-driven governance entails regulating human behavior on a global scale to achieve climate goals. This approach, while effective, poses a threat to individual freedom through coercion. The paper draws parallels to the political philosophy of Thomas Hobbes, who advocated for a Leviathan to prevent chaos, highlighting the ethical dilemma between sacrificing freedom for the greater good or finding a middle ground that preserves both.\nThe global perspective on climate change introduces a complex framework of justice-related challenges. Not all communities and nations are equally vulnerable to climate change, which underlines concerns about justice and fairness. It is necessary to consider the differential impact of climate change measures on various communities, generations, and regions. This introduces questions of global justice, highlighting the ethical responsibility to evaluate AI interventions based on their effects on different populations and geographical locations.\nThe intergenerational dimension of climate change raises ethical questions about who should bear the costs of addressing a crisis that spans multiple generations. As a current graduate student, this prompts reflection on the ethical implications of the actions taken today on the well-being of future generations.\nThe use of AI for climate action holds great potential, but it requires a conscientious approach to ethical and political challenges. Engaging with these issues involves advocating for interdisciplinary education that integrates ethical considerations into technological development. Additionally, fostering transparency within the AI community and contributing to public discussions about the ethical implications of AI applications are crucial steps in navigating the complex terrain of using AI for climate issues. The responsibility extends beyond individual actions to collective efforts that shape the ethical and political landscape of AI in the pursuit of a sustainable and just future.\n\n\n\nCoeckelbergh, M. (2020). AI for climate: Freedom, justice, and other ethical and political challenges. AI and Ethics, 1(1), 67–72. https://doi.org/10.1007/s43681-020-00007-2\nClimate Change AI. (n.d.). https://www.climatechange.ai/about\nMinevich, M. (2023, October 5). AI champions Driving New Industry Solutions for Climate Change. Forbes. https://www.forbes.com/sites/markminevich/2021/03/31/ai-champions-driving-new-industry-solutions-for-climate-change/?sh=4ad1bc274f66"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Olivia Holt",
    "section": "",
    "text": "B.S. Environmental Engineering (2020), Colorado School of Mines\nM.S. Hydrologic Science & Engineering (2021), Colorado School of Mines"
  },
  {
    "objectID": "blog/2023-11-06-myfirstpost/index.html#background",
    "href": "blog/2023-11-06-myfirstpost/index.html#background",
    "title": "Olivia Holt",
    "section": "",
    "text": "Lake Tahoe water clarity has been measured since 1967.\nClarity is measured as depth using a Secchi disk.\nCalifornia and Nevada, are actively working to restore lake clarity to its historic 97.4 feet.\nFactors known to influence year-to-year changes in clarity include the magnitude of runoff, the warming of the lake surface and the depth to which the lake mixes in the previous winter."
  },
  {
    "objectID": "blog/2023-11-06-myfirstpost/index.html#analysis",
    "href": "blog/2023-11-06-myfirstpost/index.html#analysis",
    "title": "Olivia Holt",
    "section": "Analysis",
    "text": "Analysis\n\nSimple Linear Regression model\nCall: lm(formula = Secchi_Ave ~ avg_Chla, data = joined)\nResiduals:     Min      1Q  Median      3Q     Max\n-12.144  -2.554  -0.314   2.535  18.834\nCoefficients:             Estimate Std. Error t value             Pr(&gt;|t|)     (Intercept)  23.0601     0.4660  49.487 &lt; 0.0000000000000002 ***\navg_Chla     -1.9169     0.6615  -2.898              0.00397 **\n--- Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1  Residual standard error: 4.258 on 385 degrees of freedom Multiple R-squared:  0.02135,  Adjusted R-squared:  0.01881  F-statistic: 8.398 on 1 and 385 DF,  p-value: 0.003971\n\n\n\n\n\n\n\nTime Series Analysis\nseasonality\noverall decline in in secchi depth"
  },
  {
    "objectID": "blog/2023-11-06-myfirstpost/index.html#takeaways",
    "href": "blog/2023-11-06-myfirstpost/index.html#takeaways",
    "title": "Olivia Holt",
    "section": "Takeaways",
    "text": "Takeaways\n\nChlorophyll only shows a small influence on the variance of clarity\nlook at things that interact with chl\n\ndiatoms/zooplankton\n\n\n\nReferences\nToy, A. N. (2023, April 10). Clarity/Secchi. Tahoe Environmental Research Center. https://tahoe.ucdavis.edu/secchi#:~:text=Clarity%20sinks%20in&text=In%202022%2C%20Lake%20Tahoe’s%20average,Secchi%20depth%20was%2080.6%20feet.\nWatanabe, Shohei; Schladow, Geoffrey (2022). Limnological data for Lake Tahoe seasonal and long-term clarity trend analysis report [Dataset]. Dryad. https://doi.org/10.25338/B83P8B"
  },
  {
    "objectID": "blog/Thomas_Fire_analysis/hw4-task3_blog.html#importing-libraries",
    "href": "blog/Thomas_Fire_analysis/hw4-task3_blog.html#importing-libraries",
    "title": "AQI Analysis in Santa Barbara",
    "section": "",
    "text": "Code\n#importing libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches # for creating legends\n\nimport xarray as xr\nimport rioxarray as rioxr\nimport geopandas as gpd\n\nfrom rasterio.features import rasterize #for rasterizing polygons\nfrom shapely.geometry import box\nfrom shapely.geometry import Point\n\n\nModuleNotFoundError: No module named 'xarray'"
  },
  {
    "objectID": "blog/Thomas_Fire_analysis/hw4-task3_blog.html#import-data",
    "href": "blog/Thomas_Fire_analysis/hw4-task3_blog.html#import-data",
    "title": "AQI Analysis in Santa Barbara",
    "section": "",
    "text": "loading in the both the AQI data and the california fire perimeter data\n\n\nCode\n#read in aqi 2017 data\naqi_17 = pd.read_csv(\"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip\")\n\n#read in aqi 2018 data\naqi_18 = pd.read_csv(\"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip\")\n\n# importing california fire perimeter data\nca = gpd.read_file('data/California_Fire_Perimeters_2017.shp')\n\n#reading in the .nc file\nlandcover_fp = os.path.join(os.getcwd(),'data','landsat8-2018-01-26-sb-simplified.nc')\nlandcover = rioxr.open_rasterio(landcover_fp)"
  },
  {
    "objectID": "blog/Thomas_Fire_analysis/hw4-task3_blog.html#data-exploration",
    "href": "blog/Thomas_Fire_analysis/hw4-task3_blog.html#data-exploration",
    "title": "AQI Analysis in Santa Barbara",
    "section": "",
    "text": "Code\n# looking at the head of the data\nca.head()\n\n#looking at the shape\nca.shape\n\n#looking at the head of the data\nlandcover.head()\n\n#looking at the shape of landcover\nlandcover.rio.shape\n\n#looking at the dimensions of the landcover data\nlandcover.dims\n\n#view first 5 columns of aqi 2017 data\naqi_17.head()\n\n#view first 5 columns of aqi 2018 data\naqi_18.head()\n\n#view columns of aqi 2017 data\naqi_17.columns\n\n\nIndex(['State Name', 'county Name', 'State Code', 'County Code', 'Date', 'AQI',\n       'Category', 'Defining Parameter', 'Defining Site',\n       'Number of Sites Reporting'],\n      dtype='object')"
  },
  {
    "objectID": "blog/Thomas_Fire_analysis/hw4-task3_blog.html#cleaningorganizing",
    "href": "blog/Thomas_Fire_analysis/hw4-task3_blog.html#cleaningorganizing",
    "title": "AQI Analysis in Santa Barbara",
    "section": "",
    "text": "Tidying the air quality data by combining both dataset for easier use. The raster data contains additionals bands that are not needed for this analysis. These bands were removed to improve processing.\n\n\nCode\n#combine both data sets into one dataset unsing concat\naqi = pd.concat([aqi_17,aqi_18])\n\n# re-assign the column names - .str.lower() makes them lower case\naqi.columns = aqi.columns.str.lower()\n\n#  re-assign the column names again - .str.replace(' ','_') replaces the space for _\naqi.columns = aqi.columns.str.replace(' ','_')\n\n\n\n\nCode\n#dropping the bands to make a 2d data set in order to plot it\nlandcover = landcover.squeeze().drop('band')\n\n# checking to see if the crs's match\nca.crs == landcover.rio.crs\n\n#set landcover crs to ca crs, crs='EPSG:3857'\nca = ca.to_crs(landcover.rio.crs)"
  },
  {
    "objectID": "blog/Thomas_Fire_analysis/hw4-task3_blog.html#analysis",
    "href": "blog/Thomas_Fire_analysis/hw4-task3_blog.html#analysis",
    "title": "AQI Analysis in Santa Barbara",
    "section": "",
    "text": "Filtering the air quality to the area of interest for analysis. Converting the data column to a datetime object will help when plotting the data.\n\n\nCode\n#selecting rows that equal 'santa barbara' in the county_name column\naqi_sb = aqi[aqi['county_name'] == 'Santa Barbara']\n\n#removing the four specified columns\naqi_sb = aqi_sb.drop(columns = ['state_name','county_name','state_code', 'county_code'])\n\n# covert aqi_sb.DATE column to timedate objects\npd.to_datetime(aqi_sb.date)\n\n# convert DATE column from string to timestamps\naqi_sb.date = pd.to_datetime(aqi_sb.date)\n\n#update the index tp the date column\naqi_sb = aqi_sb.set_index(aqi_sb.date)\n\n# and we get a pd.Series as ouput\naqi_sb.aqi.rolling('5D').mean()\n\n#adding a new column named 'five_day_average' into the aqi_sb dataframe\n#finding the mean of the AQI over a 5 day rolling window and inputting those values into the new column\naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean()\n\n\n\n\nCode\n#selecting the thomas fire from the california fire 2017 dataset\nthomas_fire = ca[ca['FIRE_NAME'] == 'THOMAS']"
  },
  {
    "objectID": "blog/Thomas_Fire_analysis/hw4-task3_blog.html#final-plotting",
    "href": "blog/Thomas_Fire_analysis/hw4-task3_blog.html#final-plotting",
    "title": "AQI Analysis in Santa Barbara",
    "section": "",
    "text": "The final air quality plot for Santa Barbara county, impacted by the Thomas Fire. This shows the daily AQI and a 5 day average. The next plot is a map of the fire perimeter in Southern California.\n\n\nCode\n#plotting the AQI and the rolling averaged AQI over 5 days against eachother\naqi_sb.plot(y= ['aqi','five_day_average'], title='AQI vs 5 Day Averages', color = ['purple', 'green'] )\n\n\n&lt;AxesSubplot:title={'center':'AQI vs 5 Day Averages'}, xlabel='date'&gt;\n\n\n\n\n\n\n\nCode\n# setting up a plot\nfig, ax = plt.subplots()\n\n# plotting the thomas fire on top of the landover plot\nthomas_fire.plot(ax=ax,\n                color = \"darksalmon\",\n                edgecolor= \"black\")\nlandcover[['swir22','nir08','red']].to_array().plot.imshow(robust = True)  #landcover plot\n\n#setting the title name\nax.set_title('Thomas Fire in Santa Barbara County',  fontsize=14)\n\n\nText(0.5, 1.0, 'Thomas Fire in Santa Barbara County')"
  },
  {
    "objectID": "blog/Thomas_Fire_analysis/hw4-task3_blog.html",
    "href": "blog/Thomas_Fire_analysis/hw4-task3_blog.html",
    "title": "AQI Analysis in Santa Barbara",
    "section": "",
    "text": "To visualize the impact on the Air Quality Index Thomas Fire in Santa Barbara County and to create a false color image showing the fire scar of the Thomas fire in 2017.\n\n\n\n\nData wrangling and exploration\nGeospatial data wrangling\nCreating a plot for AQI through 2017\nCreating and customizing a map\n\n\n\n\nDataset 1\nAir Quality Index (AQI) data from the US Environmental Protection Agency.\nDataset 2\nA simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data, collected by the Landsat 8 satellite.\nThis data was accessed and pre-processed in the Microsoft Planetary Computer to remove data outside land and coarsen the spatial resolution (Landsat Collection in MPC). Data should be used for visualization purposes only.\nDataset 3\nA shapefile of fire perimeters in California during 2017. The complete file can be accessed in the CA state geoportal.\n\n\n\nAirNow. (2021b). Air Quality Index (AQI) Basics. Retrieved from www.airnow.gov website: https://www.airnow.gov/aqi/aqi-basics/\nWikipedia Contributors. (2019, October 6). Thomas Fire. Retrieved from Wikipedia website: https://en.wikipedia.org/wiki/Thomas_Fire\nCalifornia Fire Perimeters (all). (n.d.). Retrieved November 29, 2023, from gis.data.ca.gov website: https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about\nMicrosoft Planetary Computer. Planetary Computer. (n.d.). https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2))\n\n\n\n\n\nCode\n#importing libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches # for creating legends\n\nimport xarray as xr\nimport rioxarray as rioxr\nimport geopandas as gpd\n\nfrom rasterio.features import rasterize #for rasterizing polygons\nfrom shapely.geometry import box\nfrom shapely.geometry import Point\n\n\nModuleNotFoundError: No module named 'xarray'\n\n\n\n\n\nloading in the both the AQI data and the california fire perimeter data\n\n\nCode\n#read in aqi 2017 data\naqi_17 = pd.read_csv(\"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2017.zip\")\n\n#read in aqi 2018 data\naqi_18 = pd.read_csv(\"https://aqs.epa.gov/aqsweb/airdata/daily_aqi_by_county_2018.zip\")\n\n# importing california fire perimeter data\nca = gpd.read_file('data/California_Fire_Perimeters_2017.shp')\n\n#reading in the .nc file\nlandcover_fp = os.path.join(os.getcwd(),'data','landsat8-2018-01-26-sb-simplified.nc')\nlandcover = rioxr.open_rasterio(landcover_fp)\n\n\n\n\n\nTidying the air quality data by combining both dataset for easier use. The raster data contains additionals bands that are not needed for this analysis. These bands were removed to improve processing.\n\n\nCode\n#combine both data sets into one dataset unsing concat\naqi = pd.concat([aqi_17,aqi_18])\n\n# re-assign the column names - .str.lower() makes them lower case\naqi.columns = aqi.columns.str.lower()\n\n#  re-assign the column names again - .str.replace(' ','_') replaces the space for _\naqi.columns = aqi.columns.str.replace(' ','_')\n\n\n\n\nCode\n#dropping the bands to make a 2d data set in order to plot it\nlandcover = landcover.squeeze().drop('band')\n\n# checking to see if the crs's match\nca.crs == landcover.rio.crs\n\n#set landcover crs to ca crs, crs='EPSG:3857'\nca = ca.to_crs(landcover.rio.crs)\n\n\n\n\n\nFiltering the air quality to the area of interest for analysis. Converting the data column to a datetime object will help when plotting the data.\n\n\nCode\n#selecting rows that equal 'santa barbara' in the county_name column\naqi_sb = aqi[aqi['county_name'] == 'Santa Barbara']\n\n#removing the four specified columns\naqi_sb = aqi_sb.drop(columns = ['state_name','county_name','state_code', 'county_code'])\n\n# covert aqi_sb.DATE column to timedate objects\npd.to_datetime(aqi_sb.date)\n\n# convert DATE column from string to timestamps\naqi_sb.date = pd.to_datetime(aqi_sb.date)\n\n#update the index tp the date column\naqi_sb = aqi_sb.set_index(aqi_sb.date)\n\n# and we get a pd.Series as ouput\naqi_sb.aqi.rolling('5D').mean()\n\n#adding a new column named 'five_day_average' into the aqi_sb dataframe\n#finding the mean of the AQI over a 5 day rolling window and inputting those values into the new column\naqi_sb['five_day_average'] = aqi_sb.aqi.rolling('5D').mean()\n\n\n\n\nCode\n#selecting the thomas fire from the california fire 2017 dataset\nthomas_fire = ca[ca['FIRE_NAME'] == 'THOMAS']\n\n\n\n\n\nThe final air quality plot for Santa Barbara county, impacted by the Thomas Fire. This shows the daily AQI and a 5 day average. The next plot is a map of the fire perimeter in Southern California.\n\n\nCode\n#plotting the AQI and the rolling averaged AQI over 5 days against eachother\naqi_sb.plot(y= ['aqi','five_day_average'], title='AQI vs 5 Day Averages', color = ['purple', 'green'] )\n\n\n&lt;AxesSubplot:title={'center':'AQI vs 5 Day Averages'}, xlabel='date'&gt;\n\n\n\n\n\n\n\nCode\n# setting up a plot\nfig, ax = plt.subplots()\n\n# plotting the thomas fire on top of the landover plot\nthomas_fire.plot(ax=ax,\n                color = \"darksalmon\",\n                edgecolor= \"black\")\nlandcover[['swir22','nir08','red']].to_array().plot.imshow(robust = True)  #landcover plot\n\n#setting the title name\nax.set_title('Thomas Fire in Santa Barbara County',  fontsize=14)\n\n\nText(0.5, 1.0, 'Thomas Fire in Santa Barbara County')"
  },
  {
    "objectID": "blog/eds223_final_blog/index.html",
    "href": "blog/eds223_final_blog/index.html",
    "title": "Houston Blackout Spatial Analysis",
    "section": "",
    "text": "What impact did the Texas power outage have on different socioeconomic communities?"
  },
  {
    "objectID": "blog/eds223_final_blog/index.html#problem-statement",
    "href": "blog/eds223_final_blog/index.html#problem-statement",
    "title": "Houston Blackout Spatial Analysis",
    "section": "",
    "text": "What impact did the Texas power outage have on different socioeconomic communities?"
  },
  {
    "objectID": "blog/eds223_final_blog/index.html#goals",
    "href": "blog/eds223_final_blog/index.html#goals",
    "title": "Houston Blackout Spatial Analysis",
    "section": "Goals",
    "text": "Goals\n“In February 2021, the state of Texas suffered a major power crisis, which came about as a result of three severe winter storms sweeping across the United States on February 10–11, 13–17, and 15–20.”\nTasks for this analysis\n\nestimating the number of homes in Houston that lost power as a result of the first two storms\ninvestigating if socioeconomic factors are predictors of communities recovery from a power outage\n\nHighlights of Analysis:\n\nloading vector/raster data\nsimple raster operations\nsimple vector operations\nspatial joins\n\n\nData\nNight lights data\nSpatial data is remotely-sensed night lights data, acquired from the Visible Infrared Imaging Radiometer Suite (VIIRS) onboard the Suomi satellite. Specifically, VNP46A1 to detect differences in night lights before and after the storm to identify areas that lost electric power.\nRoads\nTo minimize falsely identifying areas with reduced traffic as areas without power, areas near highways will be ignored. Geofabrik’s download sites will be used to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area. \nHouses\nWe can also obtain building data from OpenStreetMap from Geofabrick and prepared a GeoPackage containing only houses in the Houston metropolitan area.\nSocioeconomic\nWe cannot readily get socioeconomic information for every home, so instead we obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019.\nread in and combine the data\n\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(spData)\nlibrary(geodata)\nlibrary(spDataLarge)\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(patchwork)\n\n#reading in the data with read_stars for the night lights data\nnight_1 &lt;- read_stars(\"data/VNP46A1.A2021038.h08v05.001.2021039064328.tif\", package = \"stars\")\n\nnight_2 &lt;- read_stars(\"data/VNP46A1.A2021038.h08v06.001.2021039064329.tif\", package = \"stars\")\n\nnight_3 &lt;- read_stars(\"data/VNP46A1.A2021047.h08v05.001.2021048091106.tif\", package = \"stars\")\n\nnight_4 &lt;- read_stars(\"data/VNP46A1.A2021047.h08v06.001.2021048091105.tif\", package = \"stars\")\n\n#combining into one for each date: 02-07\nstars_1 &lt;- st_mosaic(night_1, night_2)\n\n#combining into one for each date: 02-16\nstars_2 &lt;- st_mosaic(night_3, night_4)\n\nFirst a blackout mask was created by finding the change intensity from the night light data. All locations that experienced a drop of more than 200 cm-2sr-1 were assumed to have experienced a blackout.\n\n#find the change in night lights intensity\nchange_in_intensity &lt;- stars_1 - stars_2\n\n#viewing in a plot\nplot(change_in_intensity)\n\ndownsample set to 1\n\n\n\n\n#reclassify the difference raster, with cut() and breaks\nintensity_reclass &lt;- cut(change_in_intensity, breaks = c(200, Inf), labels = \"exp. blackout\")\n\nThe data was then vectorized.\n\n#using st_as_sf() to vectorize, check geometries are valid\ninten_vec &lt;- st_as_sf(intensity_reclass) %&gt;%\n  st_make_valid(inten_vec)\n\nWith the defined region of Houston, the vectorized map from above was cropped to the area of interest.\n\n#make a data.frame x = c(coord), y = c(coords))\nbounds &lt;- data.frame(x = c(-96.5, -96.5, -94.5, -94.5), y = c(29, 30.5, 30.5, 29))\n\n#st_as_sf make coords\nhouston &lt;- bounds %&gt;% \n  st_as_sf(coords = c(\"x\", \"y\")) %&gt;% \n  st_set_crs(\"EPSG:4326\") %&gt;%\n  st_combine() %&gt;% \n  st_cast(\"POLYGON\") #turn into a polygon\n\n#bounding the lights data with houston coordinates\nhouston_lights &lt;- inten_vec[houston, ]\n\n#reprojecting using st_transform\nhouston_lights_transform &lt;- st_transform(houston_lights, crs = \"EPSG:3083\")\n\nThe roads geopackage includes data on roads other than highways. However, we can avoid reading in data we don’t need by taking advantage of st_read’s ability to subset using a SQL query.\nAreas within 200m of all highways using st_buffer were identified. Then the areas that experienced blackouts that are further than 200m from a highway were found using st_difference().\nquery &lt;- “SELECT * FROM gis_osm_roads_free_1 WHERE fclass=‘motorway’”highways &lt;- st_read(“data/gis_osm_roads_free_1.gpkg”, query = query)\n\n#loading query\nquery1 &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n#loading data\nhighways &lt;- st_read(\"data/gis_osm_roads_free_1.gpkg\", query = query1) %&gt;% \n  st_transform(3083)\n\n#using a buffer to find areas within 200m\nhwys_200 &lt;- highways %&gt;% \n  st_buffer(dist = 200) %&gt;% \n  st_union()\n\n#using st_difference to \nhwys_further &lt;- st_difference(houston_lights_transform , hwys_200)\n\nTo find homes impacted by blackouts st_read and the following SQL query to select only residential buildings were used.\nSELECT *  FROM gis_osm_buildings_a_free_1WHERE (type IS NULL AND name IS NULL)OR type in (‘residential’, ‘apartments’, ‘house’, ‘static_caravan’, ‘detached’)\n#__________________________________________________________\n\n#this query selects for residential already\nquery2 &lt;- \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE (type IS NULL AND name IS NULL)OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n#reading in the buildings data and transforming the geometries\nbuildings &lt;- st_read(\"data/gis_osm_buildings_a_free_1.gpkg\", query = query2) %&gt;% \n  st_transform(3083)\n\nfilter to homes within blackout areas using st_filter().\n\n#using filter to find homes within blackout areas\nhomes_blackout&lt;- st_filter(buildings, hwys_further)\n\n#printing the number of rows (number of homes within a blackout area)\nprint(nrow(homes_blackout))\n\n[1] 157411\n\n\nUsing st_read() to load the geodatabase layers.\n\n#using st_read() to load in gdb layers\n#reprojecting to EPSG:3080\nacs_geom &lt;- st_read(\"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer = \"ACS_2019_5YR_TRACT_48_TEXAS\") %&gt;% st_transform(3083) #transforming geometries\n\nReading layer `ACS_2019_5YR_TRACT_48_TEXAS' from data source \n  `C:\\Users\\olivi\\Documents\\MEDS\\olleholt.github.io\\blog\\eds223_final_blog\\data\\ACS_2019_5YR_TRACT_48_TEXAS.gdb' \n  using driver `OpenFileGDB'\nSimple feature collection with 5265 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -106.6456 ymin: 25.83716 xmax: -93.50804 ymax: 36.5007\nGeodetic CRS:  NAD83\n\n#reading in the income layer from the gdb\nacs_income &lt;- st_read(\"data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\", layer = \"X19_INCOME\") %&gt;% select(\"B19013e1\",\"GEOID\") #selecting the median income field B19013e1\n\nReading layer `X19_INCOME' from data source \n  `C:\\Users\\olivi\\Documents\\MEDS\\olleholt.github.io\\blog\\eds223_final_blog\\data\\ACS_2019_5YR_TRACT_48_TEXAS.gdb' \n  using driver `OpenFileGDB'\n\n\nThe income data and the census tract geometries were joined using a left_join() and st_join. Then the number of impacted tracts were found using the unique() function.\n\n#renaming the column in income data to match the correct column in the geometries data\nincome_new &lt;- acs_income %&gt;% rename(\"GEOID_Data\" = \"GEOID\")\n\n#left_join/full_join and match column names\n#joining income data to census track geom\nacs_join &lt;- left_join(acs_geom, income_new,by = join_by(GEOID_Data))\n\n#transforming the houston polygon\nhouston_3083 &lt;- st_transform(houston, 3083)\n#bounding the joined data with houston coordinates\nacs_houston &lt;- acs_join[houston_3083, ]\n\n#subsetting the blackout ares to houston area with the gdb layers\nhouston_subset &lt;- acs_houston[homes_blackout, ]\n\n#the GEO and GEO_Data columns\nspatial_join &lt;- st_join(homes_blackout, acs_join)\n\n#unique values in GEOID_Data\ncensus_n &lt;- unique(spatial_join$GEOID_Data)\n\nprint(length(census_n))\n\n[1] 754\n\n\nCreating plots to compare incomes of impacted tracts to unimpacted tracts.\nThe first plot is a map of median income by census tract, designating which tracts had blackouts and the second an third plots will compare the distributions of income in impacted and unimpacted tracts.\n\n#plotting the income and census track data first\n#polygons are colored based on median income\n#plotting the blackout homes on top of the houston area\ntm_shape(acs_join, bbox = st_bbox(hwys_further))+\n  tm_polygons(fill = \"B19013e1\", title = \"Median Income\")+\n  tm_shape(houston_subset)+\n  tm_borders(col = \"darkgreen\")+\n  tm_compass()+\n  tm_scalebar()+\n  tm_title(\"Median Income by Census Tract\")\n\nDeprecated tmap v3 code detected. Code translated to v4\n\n\n\n\n\n\n#selecting for matching values in the GEOID_Data column\nimpacted &lt;- acs_houston %&gt;% filter(GEOID_Data %in% houston_subset$GEOID_Data)\n#selecting for non-matching values in the GEOID_Data column\nunimpacted &lt;- acs_houston %&gt;% filter(!GEOID_Data %in% houston_subset$GEOID_Data)\n\n#check to make sure unique GEOID_Data values matches the impacted number of rows\nprint(length(unique(houston_subset$GEOID_Data)) == nrow(impacted))\n\n[1] TRUE\n\n#plotting the impacted homes count\nimpacted_plot &lt;- ggplot()+\n  geom_histogram(data = impacted, aes(x = B19013e1))+\n  xlab(\"Median Income\")+\n  ggtitle(\"Homes impacted by blackout\")\n\n#plotting the unimpacted homes count\nunimpacted_plot&lt;- ggplot()+\n  geom_histogram(data = unimpacted, aes(x = B19013e1))+\n  xlab(\"Median Income\")+\n  ggtitle(\"Homes unimpacted by blackout\")\n\n#using patchwork to plot them together\nimpacted_plot+unimpacted_plot\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nDiscussion\nThe histogram distributions look very similar on first glance. The y ranges are different but overall both plots are skewed to the right and unimodal. The map and the histograms do show that there is more of an impact from the storm for households with incomes less than $50,000 annually and a lower impact for blackouts for households. The limitations in the census tract data is that it is voluntary and therefore we do not know if it accurately represents the population as well as it could."
  }
]